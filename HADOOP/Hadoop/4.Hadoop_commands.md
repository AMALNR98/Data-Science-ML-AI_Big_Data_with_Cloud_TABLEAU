- start all demons
	- `start-all.sh`

- Then
	- open browser then take localhost
	- localhost:50070


- For start daemons in hdfs
	- `start-dfs.sh`

- stop demons in hdfs
	- `stop-dfs.sh`

- For start processing demons
	- `start-yarn.sh`

- For stop processing demons
	- `stop-yarn.sh`

- When we start all demons there is a chance for missing of one demon so check the demon where belongs to then, 
	- if it is in hdfs
		- `start-dnf.sh`

	- if it's in YARN 
		- `start-yarn.sh`

- For start a daemon individually for hadoop
	- `hadoop-demon.sh start "name"`

	- for YARN
	    - `yarn-daemon.sh start "name"`

- for stop a daemon individually
	- `hadoop-daemon.sh stop "name"`

``
---------------------------------
first start all			|
if all daemons didn't came      |
start componet wise             |
the do individully              |
---------------------------------
``
- hard-way
	``
	sudo rm -r /app
	sudo mkdir -R /app/hadoop/tmp
	sudo chmod -R 777 /app
	sudo chown -R hduser:hadoop /app
	ssh localhost
	hadoop namenode -format
    ``
- for find full detail and status of hadoop
	- hadoop fsck - /
		- `fsck - file system check`

- To check memory status
    - `hadoop fs -df hdfs:/`

- for check then name node working properly 
	- `hadoop balancer`
##########################################################################################################################################################################################################

 - ***Important comments***
---


- list : for list datas in HDFS
	     - `hadoop fs -ls /`
				- / ==> root of hdfs or top of hdfs (list all contents of main page)
- How to create a directory in HDFS
	- `hadoop fs -mkdir /"directory name"`

- How to create a directory inside and directory
	- `hadoop fs -mkdir /"directory-name(parent)"/"directory-name(child)"`


--------------------------------------------------------------------------------------------------------
- assignment
----------
``
 - #Luminar/coursers/bigdata/hadoop
	spark
			#python dango
			angular
			react
			flutter/dart
			testing
``
---------------------------------------------------------------------------------------------------------

- mkdir -p

    - `hadoop fs -mkdir -P /a/b/c/d`


- how to create a file


    - touchz

        - `hadoop fs -touchz /"filename"`
            - directory doesn't have replication factor   >>>>>>
- To find count of directories and files, then size
	- `hadoop fs -count /`

- no.of directories	no.of files	size
        - `hadoop fs -count /'dirctories name"`


- How to delete file 
	- `hadoop fs -rm /filename`

- How to delete a directory
	- `hadoop fs -rmdir /"filename"`         ---> when directory empty
	- `hadoop fs -rm -r /"filename"`         ---> when directory is not empty

- How to copy a file
	- 2 types of copy method
		1. hdfs to hdfs
		    - `hadoop fs -cp "source_path" "destination_path"`

		2. local to hdfs copy method
			- `hadoop fs -copyFromLocal "source_path" "destination_path"`

			- `hadoop fs -put "sourse_path" "destination_path"`

- How to display hdfs data
	- `hadoop fs -cat /filepath`


  - head is not implemented in hdfs

  - only tail commend is used in hdfs

      - `hadoop fs -tail /filename`

      ''''''''''''''''''''''''in tail we can access up 1 kilo byte of data'''''''''''''''''''''''

- How to copy an directory
	- likewise files, same command used for directory
		- `hadoop fs -cp "sourse-path" "destination-path"`

- How to copy file form hdfs to local 
----------------------------------
	- `hadoop fs -copyToLocal "source_Path" "destination_path"`
	
	- in here we can't copy file, because of no permission for local to access file 
	- so give permission
	- `sudo chmod -R 777 "Destination_path"`

	- hadoop fs -get "source-path" 'destination-path"

===================================================================================================END========================================================================================================
